################################################################################################
# Sequential Library
# This script provides the modules required for the Sequential Library
#
# Author: Arasch Lagies
#
# Firts Version: 12/6/2019
# Last Update: 08/28/2020
#
# This top-level function library contains all functions and calls required for model processing
################################################################################################
import os
import csv
from signal import signal, SIGINT
from sys import exit
import numpy as np
from tqdm import tqdm
from optimizer import * 
from inference import *
from prepParams4FPGA import *
import pickle
import matplotlib.pyplot as plt
from saveModel2txtFile import *
import logging
import time
import datetime 
try:
    import platform as pf
except:
    pass


DEFAULTLAYER     = ['Dense']
DEFAULTOPTIMIZER = 'adam'
DEFAULTLOSS      = 'categorical_crossentropy'
DEFAULTMETRICS   = ['accuracy']
DEFAULTEPOCHS    = 2
SAVEPATH         = "./Test_model.pkl"
DEFAULTMODELNAME = "logfile.log"
LOGFILEDIR       = "./modelSave"

BATCHSIZE        = 32
NUMCLASSES       = 10
LEARNINGRATE     = 0.01
IMAGEDIM         = 28               # Default image dimension assuming square images
IMAGEDEPTH       = 1                # Default image depth
BETA1            = 0.95             # The exponential decay rate for the first moment estimates (e.g. 0.9).
BETA2            = 0.99             # The exponential decay rate for the second-moment estimates (e.g. 0.999). 
                                    # This value should be set close to 1.0 on problems with a sparse gradient 
                                    # (e.g. NLP and computer vision problems).
EPSILON          = 1e-7             # Add-on to the Adam formular denominator to avoind divide-by-zero...
KERNEL_SIZE      = (3, 3)           # Default kernel size for convolutions
FILTERS          = 5                # Number of fikters applied on one layer
POOLSIZE         = (2, 2)           # Default Pool size 
STRIDEConv       = (1, 1)           # Stride in x and y direction for Convolutions
STRIDEMaxp       = (2, 2)           # Stride in x and y direction for MaxPools
CONVPADDING      = 'none'           # "Padding applied for convolution. The width of rows and columns of padding are set so the kernel center covers the entire frame. \
                                    # Available are: 'none'= No padding (this is equivalent to VALID padding in TF2), 'zeros'= padding with zeros, 
                                    # 'edge'= The edge pixels are repeated, 'wrap'= The pixels of the adjacent side are copied and wrapped around. 
MP_PADDING       = 'none'           # Padding applied for Max Pooling. 
                                    # Available options are: 'none'= No padding (this is equivalent with VALID padding in TF2), 'zeros'= padding with zeros (the added height is 
                                    # 2x (width_of_kernel-1)/2, the added width is 2x (height_of_kernel-1)/2) - 'zeros' padding is equivalent to SAME padding in TF2.
                                    # 'edge'= The edge pixels are repeated, 'wrap'= The pixels of the adjacent side are copied and wrapped around.
SCALE2BITDEPTH  = 8                 # For Accelerator: If this value is > 0 and integer multiple of 2, then the Weights and Biases of the Conv2D layer 
                                    # will be scale to the corresponding bitdepth...
CONVERTTO       = 'TwoComplement'   # For Accelerator: IF this string is 'TwoComplement' then all Weights and Biases of the Conv2D layers are 
                                    # converted to Two's Complement representation, if it is 'None' no conversion,
                                    # otherwhise the values will be converted to 7bit+1bit signage...
INPUTNORM       = 'None'            # Normalization performed on the input frame(s)...
NORMCHOICE      = 'byStd'  #'by255' # For Normalization Layer: Normalization for normalization layer...

def initializeFilter( size, scale = 1.0 ):
    stddev = scale/np.sqrt(np.prod(size))
    return np.random.normal(loc = 0, scale = stddev, size = size)

def initializeWeight( size ):
    return np.random.standard_normal(size=size) * 0.01

def terminated(signal_received, frame):
    logging.info("Processing was terminated manually...")
    exit(0)

# Set handler for manual termination (ctrl c)...
signal(SIGINT, terminated)

class LayersDef():
    def __init__(self):
        self.inputset = 0           # Flag indicating that the dimensions of the input frame have been specified (first layer)
        self.img_dim = 0            # For the frame width and height given by the 1st layer
        self.img_depth = 0          # For the channels of the 1st input layer
        self.filters_prev = 0       # This indicates the amount of frames generated by the filters of a previous Conv2D layer 
        self.fnum = []              # Number of channels
        self.out_shape = []
        # Layer numbering
        self.convN = 1
        self.maxpN = 1
        self.flatN = 1
        self.densN = 1
        self.normN = 1
        
    def Conv2D( self, input_shape=None, kernel_size=KERNEL_SIZE, filters=FILTERS, stride=STRIDEConv, 
                padding=CONVPADDING, activation=None, *args, **kwargs ):
        # Check if parameters are correct...
        if kwargs:
            raise NameError(f"[ERROR in MODEL PARAMETER] The following parameter is not recognized for 'Conv2D': {kwargs}")

        self.input_shape = input_shape 
        self.kernel_size = kernel_size
        self.filters = filters
        self.stride = stride
        self.padding = padding
        self.activation = activation
        # padding addition...
        if(padding == None or padding == 'none'):
            pad_add = 0
        else:
            pad_add = 2 * int(kernel_size[0]/2)

        ## Initializing the layer parameters
        if self.inputset == 0:
            self.fnum.append( {"Layer" : "Conv2D" + "_" + str(self.convN), "Kernels" : self.filters, "Kernel Shape" : (self.input_shape[2], self.kernel_size[0], self.kernel_size[1])} )  # layers is a list containing on pos1 the name of the layer type                                                                                                             
                                                                                                                    # and on pos 2 a tuple with the layer parameters...
            self.inputset = 1
            self.filters_prev = self.filters
            h = int((self.input_shape[0] - self.kernel_size[0])/self.stride[0]) + 1 + pad_add
            w = int((self.input_shape[1] - self.kernel_size[1])/self.stride[1]) + 1 + pad_add
            self.out_shape = [self.filters, h, w]
        else:
            self.fnum.append( {"Layer" : "Conv2D" + "_" + str(self.convN), "Kernels" : self.filters, "Kernel Shape" : (self.filters_prev, self.kernel_size[0], self.kernel_size[1] ) } )
            h = int((self.out_shape[1] - self.kernel_size[0])/self.stride[0]) + 1 + pad_add
            w = int((self.out_shape[2] - self.kernel_size[1])/self.stride[1]) + 1 + pad_add
            self.filters_prev = self.filters
            self.out_shape = [self.filters, h, w]
        self.convN += 1     # Counting up the Conv layer counter...

    def MaxPool2D( self, pool_size=POOLSIZE, strides=STRIDEMaxp, padding=MP_PADDING, *args, **kwargs ):
        # Check if parameters are correct...
        if kwargs:
            raise NameError(f"[ERROR in MODEL PARAMETER] The following parameter is not recognized for 'MaxPool2D': {kwargs}")

        self.pools = pool_size
        self.padding = padding
        self.stride = strides

        h = int((self.out_shape[1] - self.pools[0])/self.stride[0]) + 1
        w = int((self.out_shape[2] - self.pools[1])/self.stride[1]) + 1
        self.out_shape = [self.filters, h, w]
        self.fnum.append({"Layer" : "MaxPool2D" + "_" + str(self.maxpN), "Pool Size" : self.pools, "Padding" : self.padding})
        self.maxpN += 1   # Counting up the MaxPool layer counter...

    def Flatten( self, input_shape=None, *args, **kwargs ):
        # Check if parameters are correct...
        if kwargs:
            raise NameError(f"[ERROR in MODEL PARAMETER] The following parameter is not recognized for 'Flatten': {kwargs}")

        self.fnum.append({"Layer" : "Flatten" + "_" + str(self.flatN)})
        if self.inputset == 0:
            self.out_shape = input_shape[0] * input_shape[1] * input_shape[2]
            self.inputset = 1
        else:
            self.out_shape = self.out_shape[0] * self.out_shape[1] * self.out_shape[2]
        self.flatN += 1      # Counting up the Flatten layer counter...

    def Dense( self, input_shape=None, units_out=100, activation=None, *args, **kwargs ):
        # Check if parameters are correct...
        if kwargs:
            raise NameError(f"[ERROR in MODEL PARAMETER] The following parameter is not recognized for 'Dense': {kwargs}")

        self.units = units_out
        self.activation = activation
        if self.inputset == 0:
            self.inShape = input_shape[0] * input_shape[1] * input_shape[2]                                                                  # Calculate the size of the input vector
            self.fnum.append({"Layer" : "Dense" + "_" + str(self.densN), "Output Neurons" : self.units, "Input Shape" : self.inShape} )      # assuming a flattened input
            self.inputset = 1
        else:
            self.inShape = self.out_shape
            self.fnum.append( {"Layer" : "Dense" + "_" + str(self.densN), "Output Neurons" : self.units, "Input Shape" : self.inShape} )
        self.out_shape = self.units
        self.densN += 1        # Counting up the Dense layer counter...

    def Normalize( self, norm=NORMCHOICE, *args, **kwargs ):
        # Check if parameters are correct...
        if kwargs:
            raise NameError(f"[ERROR in MODEL PARAMETER] The following parameter is not recognized for 'Dense': {kwargs}")

        self.inShape = self.out_shape
        self.fnum.append({"Layer": "Normalize" + "_" + str(self.normN), "Norm Choice": norm, "Input Shape": self.inShape, "Output Neurons": self.out_shape})
        self.normN += 1       # Counting up the Normalization layer counter...

    def __getattr__(self, name, *args, **kwargs):
        raise Exception(f"[MODEL ERROR] You have following unknown layer in your model: {name}")


class Sequential():
    def __init__( self, toTrain=True, inputNorm=INPUTNORM, scaleTo=SCALE2BITDEPTH, convertTo=CONVERTTO, loggingPath=LOGFILEDIR, model_name=None, logfile=LOGFILEDIR, *args, **kwargs ):
        self.params     = tuple()                                                              # Parameter list
        self.classes    = NUMCLASSES
        self.cost       = []    
        self.layers     = LayersDef()                                                          # Create an instance of the LayersDef class...
        self.prep       = prep4FPGA(convert_to=convertTo, bits=scaleTo)                        # Scaling and conversion functions ...
        self.scaleTo    = scaleTo
        self.convertTo  = convertTo
        self.inputNorm  = inputNorm
        self.model_name = DEFAULTMODELNAME if model_name is None else model_name
        base = os.path.splitext(self.model_name)[0]
        self.model_name = os.path.join(loggingPath, base + datetime.datetime.now().strftime("%d-%m-%Y_%I-%M-%S_%p") + '_logfile.log')
        format="%(asctime)s: %(message)s"
        logging.basicConfig(filename=self.model_name, format=format, level=logging.INFO, datefmt="%H:%M:%S")
        logging.info("Running the Axiado Machine Learning Library \n")
        logging.info(f"Date: {datetime.date.today()}")
        logging.info(f"Model is being first trained: {toTrain}. \n")
        try:
            logging.info("Information about the computer and the OS the training was performed on:")
            logging.info(f"    System: {pf.system()}.")
            logging.info(f"    Version: {pf.version()}.")
            logging.info(f"    Machine: {pf.machine()}.")
            logging.info(f"    Processor: {pf.processor()}. \n")
        except:
            pass
        
    def Add( self,  *args, **kwargs ):
        # First in the params list is the input frame and its dimensions...
        if not self.params and self.layers.input_shape != None:
            self.params += ( {"Layer" : "Input Frame", "Frame Shape" : self.layers.input_shape, "Normalization of Input Frame" : self.inputNorm}, ) 
            logging.info("-------------------------------- Model Layers ------------------------------------")
            logging.info(f"Layer : Input Frame, Frame Shape : {self.layers.input_shape}, Normalization of Input Frame: {self.inputNorm}.") 
        #-----------------------------------------------------------------------------------------------------------------------
        # Initialize the random kerel values and set biases to 0
        # This needs to be done for each layer type individually
        if self.layers.fnum[-1].get("Layer")[:6]   ==   "Conv2D":
            self.params += (self.layers.fnum[-1],)
            self.params[-1]["Weights"]    =    initializeFilter((self.layers.fnum[-1].get("Kernels"),) + self.layers.fnum[-1].get("Kernel Shape"))     # Kernel initialization and list update
            self.b = np.zeros((self.layers.fnum[-1].get("Weights").shape[0],1))             # Bias initialization
            self.params[-1]["Biases"]     =    self.b                                       # Add biases to the list
            self.params[-1]["Stride"]     =    self.layers.stride                           # Add the stride tuple to the list
            self.params[-1]["Padding"]    =    self.layers.padding                          # Padding for convolution
            self.params[-1]["Activation"] =    self.layers.activation                       # Add the activation name to the list
                                                                                            # More items are added to this dict in the optimizer
            logging.info(f'Layer : {self.layers.fnum[-1].get("Layer")[:6]}, Weights : {self.params[-1]["Weights"].shape}, Biases: {self.params[-1]["Biases"].shape}, Stride: {self.layers.stride}, Padding: {self.layers.padding}, Activation: {self.layers.activation}.') 
        #-----------------------------------------------------------------------------------------------------------------------
        elif self.layers.fnum[-1].get("Layer")[:9] ==   "MaxPool2D":
            self.params += (self.layers.fnum[-1],)
            self.params[-1]["Stride"] = self.layers.stride                                  # Add the stride tuple to the list
                                                                                            # More itens are added to this layer in the optimizer
            logging.info(f'Layer : {self.layers.fnum[-1].get("Layer")[:9]}, Stride : {self.params[-1]["Stride"]}.') 
        #-----------------------------------------------------------------------------------------------------------------------
        elif self.layers.fnum[-1].get("Layer")[:7] ==   "Flatten":
            self.params += (self.layers.fnum[-1],)                                          # Flatten gets only the layer name
                                                                                            # More items are added to this layer in the optimizer
            logging.info(f'Layer : {self.layers.fnum[-1].get("Layer")[:7]}.') 
        #-----------------------------------------------------------------------------------------------------------------------
        elif self.layers.fnum[-1].get("Layer")[:5] ==   "Dense":
            self.params += (self.layers.fnum[-1],)
            self.params[-1]["Weights"]    = initializeWeight((self.layers.fnum[-1].get("Output Neurons"), self.layers.fnum[-1].get("Input Shape")) )    # Weight initialization and list update
            self.b = np.zeros((self.layers.fnum[-1].get("Weights").shape[0],1))             # Bias initilization
            self.params[-1]["Biases"]     =    self.b                                       # Add bises to the list
            self.params[-1]["Activation"] =    self.layers.activation                       # Add the activation name to the list
                                                                                            # More items are added to this layer in the optimizer
            logging.info(f'Layer : {self.layers.fnum[-1].get("Layer")[:5]}, Weights : {self.params[-1]["Weights"].shape}, Biases: {self.params[-1]["Biases"].shape}, Activation: {self.params[-1]["Activation"]}.') 
        #-----------------------------------------------------------------------------------------------------------------------
        elif self.layers.fnum[-1].get("Layer")[:9] == "Normalize":
            self.params += (self.layers.fnum[-1],)                                          # Normalize gets here only the name of the layer
                                                                                            # More items are added to this layer in the optimizer
            logging.info(f'Layer : {self.layers.fnum[-1].get("Layer")[:9]}.') 
        #-----------------------------------------------------------------------------------------------------------------------
        else:
            logging.exception(f"[MODEL ERROR] You have following unknown layer in your model: {self.layers.fnum[-1].get('Layer')}")
            raise Exception(f"[MODEL ERROR] You have following unknown layer in your model: {self.layers.fnum[-1].get('Layer')}")
        #-----------------------------------------------------------------------------------------------------------------------
                     
    def Compile( self, optimizer = None, loss = None, metrics = None, lr = None, beta1 = None, beta2 = None, epsilon = EPSILON):
        """ 
        - Initialize all parametrs
        - Initialize gradients and momentum, RMS params
        """
        logging.info("----------------------------------------------------------------------------------")
        self.optimizer = DEFAULTOPTIMIZER if optimizer is None else optimizer
        self.loss = DEFAULTLOSS if loss is None else loss
        self.metrics = DEFAULTMETRICS if metrics is None else metrics                       # Not used at this point...
        self.lr = LEARNINGRATE if lr is None else lr
        self.beta1 = BETA1 if beta1 is None else beta1
        self.beta2 = BETA2 if beta2 is None else beta2  
        self.epsilon = epsilon
        logging.info(f'Optimizer: {self.optimizer}, Loss Function: {self.loss}, Metrics: {self.metrics}, Learning Rate: {self.lr}, Beta1: {self.beta1}, Beta2: {self.beta2}.')     


    def Fit( self, x_train = None, y_train = None, img_dim = None, img_depth = None, 
            num_epochs = None, batch = None, num_classes = None, parallel=False ):
        """
        - Forward Operation
        - Loss calculation
        - Backward calculation
        """
        self.img_dim = IMAGEDIM if img_dim is None else img_dim
        self.img_depth = IMAGEDEPTH if img_depth is None else img_depth
        self.num_epochs = DEFAULTEPOCHS if num_epochs is None else num_epochs
        self.batch_size = BATCHSIZE if batch is None else batch
        self.num_classes = NUMCLASSES if num_classes is None else num_classes
        logging.info(f'--- Training Parameters ---')
        logging.info(f'Multiprocessing on: {parallel}.')
        logging.info(f'Image Dim: {self.img_dim}, Image Depth: {self.img_depth}, Number of Epochs: {self.num_epochs}, Batch Size: {self.batch_size}, Number of Classes: {self.num_classes}.') 

        if x_train is None:
            print("[ERROR] Did not receive any training data")
            exit(0)
        if y_train is None:
            print("[ERROR] Did not receive any training labels")
            exit(0)
        self.x_train = x_train
        self.y_train = y_train
        self.train_data = np.hstack((self.x_train, self.y_train))
        np.random.shuffle(self.train_data)

        print(f"[INFO] LR: {self.lr}, Batch Size: {self.batch_size}, Number of epochs: {self.num_epochs}.")
        # Run loop over the given epochs...
        startTime = time.time()                    # Used to time the training procedure...
        for _ in range(self.num_epochs):
            np.random.shuffle(self.train_data)
            batches = [self.train_data[k:k + self.batch_size] for k in range(0, self.train_data.shape[0], self.batch_size)]
            # Run loop showing progressbar and calling optimization function...
            t = tqdm(batches)

            for x, batch in enumerate(t):
                self.params, self.cost = adamGD(batch, self.num_classes, self.lr, self.img_dim, self.img_depth, self.beta1, self.beta2, self.epsilon, self.params, self.cost, parallel=parallel)
                t.set_description("Cost: %.2f" % (self.cost[-1]))
        logging.info('---------------------------')
        endTime = time.time()                      # Used to time the training process...
        logging.info(f"The training took {endTime - startTime} seconds, or {datetime.timedelta(seconds = (endTime - startTime))}.")

        # Plot cost...
        plt.plot(self.cost, 'r')
        plt.xlabel('# Iterations')
        plt.ylabel('Cost')
        plt.legend('Loss', loc='upper right')
        plt.show()      


    def Save_Model( self, save_path = SAVEPATH, scaleTo=SCALE2BITDEPTH, convertTo=CONVERTTO ):
        """ 
        - Function to save the trained weights and biases
        """
        # Praparation of Conv2D layers for FPGA...                                 
        # Add scaling and conversion choices to Conv2D layer...
        print(f"[INFO] Saving model to pickle file {save_path}.")
        for i, layer in enumerate(self.params):
            if layer.get("Layer")[:6] == 'Conv2D':                                              # Only scaling Conv2D layers for FPGA accelerator
                if self.scaleTo > 0 and (self.scaleTo % 2)==0:                                  # Only do scaling and conversion if a conversion factor other than 0 was chosen 
                                                                                                # and if the conversion factor is a multiple of 2 (binary)
                    self.prep.ReadWB(params=layer)
                    weightSC, biasSC, scale = self.prep.ScaleWeightsBiases()
                    weightB, biasB, scale   = self.prep.ConvertNums()
                    self.params[i]["Bit Depth"]      = scaleTo                                  # Bit depth of the scaling...
                    self.params[i]["Scale Factor"]   = scale                                    # scale is the factor used to normalize values to 8-bit(e.g.)
                    self.params[i]["Scaled Weights"] = weightSC
                    self.params[i]["Binary Weights"] = weightB
                    self.params[i]["Scaled Biases"]  = biasSC
                    self.params[i]["Binary Biases"]  = biasB
                    logging.info(f'Converted the parameters for the layer #{i} named {layer.get("Layer")[:6]} to binary values with a bit depth of {self.params[i]["Bit Depth"]} bits, using a scaling factor of {self.params[i]["Scale Factor"]}.')
                    logging.info(f'The converted Weight-Matrix has a shape of {self.params[i]["Binary Weights"].shape}, and the converted Biases have a shape of {self.params[i]["Binary Biases"].shape}.')
        self.cost.insert(0, "Cost History")         # Add at location 0 the name of the cost list...
        to_save = [self.params, self.cost]
        with open(save_path, 'wb') as file:
            pickle.dump(to_save, file)
        logging.info(f'Saved pickled Weights and Biases in original format, in scaled version and in binary form (to be processed by Accelerator hardware) in the file {save_path}.')


    def Load_Model( self, save_path = SAVEPATH ):
        """
        - Function to load saved weights and biases from disc...
        """
        print(f"[INFO] Loading model file {save_path}.")
        self.params, self.cost = pickle.load(open(save_path, 'rb'))
        # Plot the cost from the loaded model...
        # plt.plot(self.cost, 'r')
        # plt.title("Cost of the loaded model.")
        # plt.xlabel("# Iterations")
        # plt.ylabel("Cost")
        # plt.legend('Loss', loc="upper right")
        # plt.show()

        # Instantiate inference
        self.infer = inference(self.params)
        return self.params

    def Save2Binary( self, save_path = SAVEPATH ):
        """
        - Function that saves the model and the trained weights and biases in binary file format...
        """
        print("[INFO] Saving the model as a Numpy binary file...")
        to_save = [self.params, self.cost]
        bin_path = os.path.splitext(save_path)[0]
        np.save(bin_path, to_save)
        logging.info(f'Saved in a Binary file the Weights and Biases in original format, in scaled version and in binary form (to be processed by Accelerator hardware) in the file {bin_path}.')


    def Save2Text( self, save_path = SAVEPATH ):
        """
        - Function that saves the model and the trained weights and biases in plaintext format...
        """
        print("[INFO] Saving the model as a CSV file...")
        # Call the formatting function...
        saveFormatted(save_path=save_path, params=self.params, cost=self.cost, scaleTo=self.scaleTo, convertTo=self.convertTo)
        logging.info(f'Saved in a text file the Weights and Biases in original format, in scaled version and in binary form (to be processed by Accelerator hardware) in the file {save_path}.')
                             

    def Predict( self, frame ):
        """
        - Inference with trained model (Format; 'numpy.ndarray')
        """
        return self.infer.forward(frame)
        

    def Evaluate( self, testX, y_dash, img_depth, img_dim ):
        """
        - Evlauate trained model with training data 
        - Plot evaluation reports
        """
        startTime = time.time()                 # Used to time the evaluation process...
        # Test image dimensions
        self.img_dim = img_dim
        self.img_depth = img_depth
        # Combine test data with labels
        test_data = np.hstack((testX,y_dash))
        # Reshape the data
        testX = test_data[:,0:-1]
        testX = testX.reshape(len(test_data), self.img_depth, self.img_dim, self.img_dim)
        y = test_data[:,-1]

        corr = 0
        digit_count = [0 for i in range(self.classes)]
        digit_correct = [0 for i in range(self.classes)]

        print()
        print("[INFO] Computing accuracy over test set:")

        t = tqdm(range(len(testX)), leave=True)

        for i in t:
            x = testX[i]
            pred, prob = self.Predict(x)
            digit_count[int(y[i])] += 1
            if pred == y[i]:
                corr += 1
                digit_correct[pred] += 1

            t.set_description("Acc:%0.2f%%" % (float(corr/(i+1))*100))

        endTime = time.time()               # Used to time the evaluation process...
        logging.info(f"The Evaluation of this model took {endTime - startTime} seconds, or {datetime.timedelta(seconds = (endTime - startTime))}.")
        print("Overall Accuracy: %.2f" % (float(corr/len(test_data)*100)))
        logging.info("Overall Accuracy: %.2f" % (float(corr/len(test_data)*100)))
        x = np.arange(self.classes)
        digit_recall = [x/y for x,y in zip(digit_correct, digit_count)]
        plt.xlabel('Digits')
        plt.ylabel('Recall')
        plt.title('Recall on Test Set')
        plt.bar(x,digit_recall)
        plt.show()