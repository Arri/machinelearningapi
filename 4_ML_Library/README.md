# Machine Learning Library Based on Numpy

## Background

CNN models can be trained using one of the many publicly available machine learning libraries, such as the ones developed by the large IT companies:
 * FaceBook's PyTorch
 * Microsoft's ML.NET
 * Google's TensorFlow

 Unfortunately, parts of these libraries are either proprietary or require proprietary hardware such a NVIDIA GPUs to run at acceptable speeds. 

 The model weights and biases generated by these libraries are trained using propriatary optimization routines that would have to be replicated in a custom   CNN accelerator for the results to be consistent. This might be not always possible, depending on the   CNN accelerator architecture.

 To overcome these issues, this   Machine-Learning library was created. With this library we have full control of the processing of the data. We can add training acceleration for many different hardware types, by e.g. using multiprocessing (is implemented) to spread the processing over all available CPU cores or OpenCL for deployment of the library on e.g. other GPUs then the ones from NVIDIA.

 This library also allows taking all or parts of the trained parameter of a model and to e.g. scale and convert them into binary format for easy transfer on experimental FPGA designs.

 This library is designed to be syntactically very similar to the TensorFlow 2.x Keras' Sequential API.
 As an example, to build a simple CNN model using TensorFLow 2 Keras Sequential API requires following code in Python:

 > model = tf.keras.models.Sequential() <br />
 > model.add(tf.keras.layers.Conv2D(input_shape=(28,28,1), kernel_size=(3,3), filters=8, strides=(1,1), padding='valid', activation='relu')) <br />
 > model.add(tf.keras.layers.Conv2D(kernel_size=(3,3), filters=8, strides=(1,1), padding='valid', activation='relu')) <br />
 > model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(1,1), padding='valid')) <br />
 > model.add(tf.keras.layers.Flatten()) <br />
 > model.add(tf.keras.layers.Dense(units=128, activation='relu')) <br />
 > model.add(tf.keras.layers.Dense(units=10, activation='softmax')) <br />

This model has first 2 convolution layers (with the various required specifications for the convolution). Next the frame size is reduced with a MaxPooling layer. The frame is then flattened and sent through a dense layer. Finally, there is an output dense layer with 10 output classes. <br />
Note that in this *modern* API all layer specifications, including the activation function can be specified in the *adding* of the layer. <br /> 
The same model can be designed using the   Machine-Learning library with the following syntax:

 > model = Sequential(inputNorm=innorm, loggingPath=SAVEMODELPATH, model_name=save_model) <br />
 > model.Add(model.layers.Conv2D(input_shape=(28,28,1), kernel_size=(3,3), filters=8, stride=(1,1), padding='none', activation='relu')) <br />
 > model.Add(model.layers.Conv2D(kernel_size=(3,3), filters=8, stride=(1,1), padding='none', activation='relu'))  <br />
 > model.Add(model.layers.MaxPool2D(pool_size=(2,2), strides=(1,1), padding='none')) <br />
 > model.Add(model.layers.Flatten()) <br />
 > model.Add(model.layers.Dense(units_out=128, activation='relu')) <br />
 > model.Add(model.layers.Dense(units_out=10, activation='softmax')) <br />

Here it was decided to include the applied input norm, a file-name for logging and a file-name, where to save the model file, in the call of the *Sequential* class. <br />
Similar to the TensorFLow API, the first layer gets the dimensions of the input frame. <br />
Then each *Conv2D* layer gets as input arguments the kernel size, the amount of kernels (filters) to apply, the stride, the padding and the activation function to be applied after the convolution operation. <br />
*MaxPooling* specifications are similar to the Conv2D parameters. <br />
The *Flatten* layer converts the 2D array to a 2D vector. <br />
The *Dense* (Fully Connected) layer gets as input the number of neurons it has (*units_out*) and the activattion function to be applied. <br />

After specifying the model architecture, the designer needs to specify the optimizer, the loss-function and other metrics to use. These specifications are required for the training process, specifically for how the forward-pass and the backward-pass are being calculated.

This specification step is done in TensorFlow in the compilation call, e.g. as:

> model.compile(optimizer = 'adam', 
                loss = tf.keras.losses.categorical_crossentropy(from_logits=False),
                metrics = ['accuracy'])

The same is done in the   Machine-Learning library with:

> model.Compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'], lr = None, beta1 = None, beta2 = None)

Finally, to start the model run one needs to specify, what the training frames and their labels are (e.g. as Python lists), what the batch-size is for each training step and over how many epochs the model should be trained.

The command to start the training with these parameters is in TensorFlow:

> model.fit(x_train, y_train, batch_size=64, epochs=100)

Using  's Machine-Learning API the call is following:

> model.Fit(x_train = trainX, y_train = trainY, img_dim = 28, img_depth = 1, num_epochs = 100, batch = 64, num_classes = 10, parallel=parallelProcess)

 's Machine Learning library then saves all of the parameters of the model, including all of the details of the model architecture in several different formats:

* Binary for faster read-in in a software-based inference.
* Plane text for reading of some or all of the parameters, so some or all of them can be e.g. converted into other formats.
* As a csv file, that can be e.g. loaded into Excel to easily examine the data.



## Riquired Packages:

The script uses besides the distribution Python 3.x (>3.5) packages following additional packages:
 - numpy
 - matplotlib
 - pandas
 - tqdm

These can be automatically installed on your Windows or Linux system using the file requirements.txt.
    If working in an Anaconda environment install packages by typing in an Anaconda terminal within a new virtual environment:

>  conda install --file requirements.txt

 In a standard pip virtual environment type:

 > pip install --upgrade -r requirements.txt

In addition, if data e.g. from the Tensorflow data base is used for training and testing, then Tensorflow should be installed with
 
 > pip install tensorflow


## Description: 

Sequential Machine Learning Library similar to Tensorflow 2.x Keras Sequential-API for model design, 
training, evaluation and deployment.

This Library can be used to train a model on labeled data (supervised learning). Also available are functions to evaluate a model and to run single inferences on provided data.
    
The general call to run a training on the provided example datasets (pickled or from tensorflow datasets) is:
 
 > python train_cnn.py save_filename.pkl

### train_cnn.py:

The file *train_cnn.py* can be used as an example for preparing a training, designing the model, its layers and its composition, and then running the training of the model.

train_cnn.py also shows in the lines 133 to 143 the available options in which format to save the trained model parameter.

train_cnn.py performs after the training a general evaluation of the trained model (lines 144 to 160), by running inference on all of the testing data and presenting the resulting statistics.

train_cnn.py generates per run also a log file which is saved by default in the same folder as the trained model parameter. The log file contains details about the model architecture, the machine the model was trained on and other parameters chosen for the training.

Lines 114 to 134 is the block where the model layers are designed. 
Following is a list of available layers and of the arguments that can be passed to them:
1. **Conv2D**
    
    a. **input_shape**: Assuming the first layer is a Conv2D layer, this first layer needs to get the dimensions of the input frames. The sequence is *(depth, height, width)*.<br />

    b. **kernel_size**: Tuple/list of 2 integers, specifying the height and width of the 2D convolution window. The 2 numbers should be odd, such as (3,3), (5,5) etc.<br />

    c. **filters**: (Also known as kernels, having each as many weights as specified in **kernel_size** + biases) Integer, the dimensionality of the output space. For the first convolution layer, this is the total amount of filters applied on the input frame and the total number of the output frames. For subsequent Conv2D layers this is the number of filters applied on each output frame of the previous layer. At the end of these subsequent Conv2D layers, the results of the filters applied on one frame are summed up. Hence the result of these subsequent Conv2D layers are as many frames as specified for **filters**. But under the hood there are (**filters** of previous Conv2D layer x **filters** of this Conv2D layer) kernels and biases generated.<br />

    d. **stride**: A tuple/list of 2 integers, specifying the strides of the convolution along the height and width.<br />

    e. **padding**: Padding along the edges of the frame. Padding is always done along row and column  edges of each frame. Padding width along these edges is the integer of half the size of **filters** for rows and for columns. Available are the choices:<br />

        i. **'none'**: No padding. 
        ii. **'zeros'**: Padding all around the frames with zeros. 
        iii. **'edge'**: Padding by repeating the edge values of the frame. 
        iiii. **'wrap'**: Padding by repeating the values of the adjacent edge of the frame. 

    d. **activation**: The activation function applied to each pixel after the convolution operation.<br /> Currently available are:<br />

        i. **'relu**: This returns the standard ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor.
        ii. **'softmax'**: The elements of the output vector are in range (0, 1) and sum to 1. Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution. The softmax of each vector x is calculated by exp(x)/tf.reduce_sum(exp(x)).

2. **MaxPool2D**

    a. **pool_size**: Tuple of two integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window.

    b. **strides**: Tuple of 2 integers. Stride values. Specifies how far the pooling window moves for each pooling step.

    c. **padding**: Following choices available:<br />

        i. **'none'**: No padding. 
        ii. **'zeros'**: Padding all around the frames with zeros. 
        iii. **'edge'**: Padding by repeating the edge values of the frame. 
        iiii. **'wrap'**: Padding by repeating the values of the adjacent edge of the frame. 

3. **Flatten**: Flattens the input from 2D array to 1D vector. Does not affect the batch size.

4. **Dense**: (Or fully connected layer) Implements the operation output = activation(dot(input, kernel) + bias), where activation is the element-wise activation function passed as the *activation* argument, *kernel* is a weights matrix created by the layer, and *bias* is a bias vector created by the layer. <br />
Arguments: <br />

    a. **units_out**: Positive integer, dimensionality of the output space.

    b. **activation**: Activation function to use. If not specified, no activation is applied (i.e. "linear" activatio: *a(x) = x*).

5. **Normalize**: Normalize the activations of the previous layer at each batch. Options for *norm* are: <br />

    a. **by255**: Divide output values by 255

    b. **byStd**: Substract the mean of the training set data. Then divide by the standard deviation of the training set data.


## Options for running train_cnn.py:

1. If a trained model is available then training can be skipped by adding the flag '--train no' to the call. In this case the provided model is used to
only run the evaluation process. The call for this case is:

 > python train_cnn.py **save_filename.pkl** --train **no**

2. If you want to provide your own training and testing data, make sure the data and labels are each saved either in pickled files or 
 as .gz files. Put training data and labels files in the folder './data/train/' and the testing data and labels files in the folder './data/test/'.
 You will also have to provide your training data size and the frame sizes (assuming here width=height) in your training set:
 
 > python train_cnn.py **save_filename.pkl** --trainx **filename1** --trainy **filename2** --testx **filename3** --testy **filename4** --size **Number of training frames** --testsize **Number of test frames** --imgWidth **width**

## Guide for creating a model:
 
- The training function needs to import the files *sequential.py* and *loadData.py*.
- The file *loadData.py* contains the functions required for reading in the training and testing data. This file might need to be modified to accommodate custom data collections.
- The file *sequential.py* is the main file containing the functions to prepare the data and the model for training. It collects all data and parameters in the tuple of dictionaries "self.params".

.... Further details TBD

  

**Author: Arasch Lagies** <br />
**  Corp.** <br />
**First Verion: Dec 2, 2019** <br />
**Last Update: 09/01/2020** <br />